{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Using GPU: {gpus}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_features = [\n",
    "    \"b0p\", \"b1p\", \"b2p\", \"b3p\", \"b4p\", \"b5p\", \"b6p\", \"b7p\", \"b8p\", \"b9p\",\n",
    "    \"b0q\", \"b1q\", \"b2q\", \"b3q\", \"b4q\", \"b5q\", \"b6q\", \"b7q\", \"b8q\", \"b9q\",\n",
    "    \"a0p\", \"a1p\", \"a2p\", \"a3p\", \"a4p\", \"a5p\", \"a6p\", \"a7p\", \"a8p\", \"a9p\",\n",
    "    \"a0q\", \"a1q\", \"a2q\", \"a3q\", \"a4q\", \"a5q\", \"a6q\", \"a7q\", \"a8q\", \"a9q\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GANConfig:\n",
    "    batch_size: int = 64\n",
    "    z_dim: int = 100\n",
    "    lob_dim: int = 40\n",
    "    epochs: int = 100\n",
    "    learning_rate_d: float = 0.0002\n",
    "    learning_rate_g: float = 0.0002\n",
    "    beta1: float = 0.5 \n",
    "    beta2: float = 0.999\n",
    "    label_smoothing: float = 0.1\n",
    "    generator_target_prob: float = 0.65\n",
    "\n",
    "config = GANConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess LOB data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The legnth for the dataframe is 189760\n"
     ]
    }
   ],
   "source": [
    "file_path = \"E:\\DSA5204 Project\\DSA5204-Group-9\\lob\\BTCUSDT-lob.parq\"\n",
    "df = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "\n",
    "print(f\"The legnth for the dataframe is {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=lob_features).sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "lob_data = scaler.fit_transform(df[lob_features].values).astype(np.float32) #(5000, 40)\n",
    "lob_dataset = tf.data.Dataset.from_tensor_slices(lob_data).batch(config.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of lob_dataset: (64, 40)\n"
     ]
    }
   ],
   "source": [
    "for batch in lob_dataset.take(1):\n",
    "    print(f\"Dimension of lob_dataset: {batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Generator model with financial constraints\n",
    "def build_generator():\n",
    "    inputs = layers.Input(shape=(config.z_dim,)) #Takes in a random noise vector of size z_dim and output a tuple. Tuple ensures 1D input and not a scalar\n",
    "    x = layers.Dense(1024, activation='relu')(inputs) #the first set of parentheses initializes the layer, and the second set applies it to the input tensor.\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dense(config.lob_dim, activation='tanh')(x)\n",
    "    \n",
    "    # Apply penalties using Lambda layers\n",
    "    penalties = layers.Lambda(lambda x: tf.nn.softplus(-x))(x)  # Ensure non-negative prices and quantities\n",
    "    bid_prices = x[:, :10]\n",
    "    ask_prices = x[:, 20:30]\n",
    "    bid_diff = bid_prices[:, :-1] - bid_prices[:, 1:]\n",
    "    ask_diff = ask_prices[:, 1:] - ask_prices[:, :-1]\n",
    "    \n",
    "    # Fix: Padding to maintain shape consistency\n",
    "    bid_diff_padded = layers.Lambda(lambda x: tf.pad(x, [[0, 0], [0, 1]]))(bid_diff)\n",
    "    ask_diff_padded = layers.Lambda(lambda x: tf.pad(x, [[0, 0], [1, 0]]))(ask_diff)\n",
    "    penalties = layers.Concatenate(axis=1)([penalties, bid_diff_padded, ask_diff_padded])\n",
    "    \n",
    "    #Get the max_bid and max_ask prices\n",
    "    max_bid = layers.Lambda(lambda x: tf.reduce_logsumexp(x, axis=1, keepdims=True))(bid_prices)\n",
    "    min_ask = layers.Lambda(lambda x: -tf.reduce_logsumexp(x, axis=1, keepdims=True))(ask_prices)\n",
    "    \n",
    "    penalties += layers.Lambda(lambda x: tf.nn.softplus(x))(max_bid - min_ask)\n",
    "    penalty_score = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(penalties) #penalty score in separate variable.\n",
    "    \n",
    "    model = models.Model(inputs, [x, penalty_score])\n",
    "    \n",
    "    # Print the model output dimensions\n",
    "    print(f\"Generator Model Output Shapes:\")\n",
    "    print(f\" - Generated LOB Data: {x.shape}\")\n",
    "    print(f\" - Penalty Score (Summed Constraints): {penalties.shape}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator outputs a model with dimension:\n",
    "\n",
    "Generated LOB shape: (64, 40)\n",
    "\n",
    "Penalty shape: (64,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch Discrimination code\n",
    "class MinibatchDiscrimination(layers.Layer):\n",
    "    \"\"\"Minibatch discrimination layer to prevent mode collapse\"\"\"\n",
    "\n",
    "    def __init__(self, num_kernels=100, dim_per_kernel=5, **kwargs):\n",
    "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
    "        self.num_kernels = num_kernels\n",
    "        self.dim_per_kernel = dim_per_kernel\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #Defines a trainable weight (self.kernel) of shape (input_dim, num_kernels, dim_per_kernel)\n",
    "        self.input_dim = input_shape[1] #Stores the number of features in the input tensor.\n",
    "        kernel_shape = (self.input_dim, self.num_kernels, self.dim_per_kernel) #Shape of the kernel tensor\n",
    "        initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=kernel_shape,\n",
    "            initializer=initializer,\n",
    "            name='kernel',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(MinibatchDiscrimination, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #Computes a projection of inputs using self.kernel.\n",
    "        #If inputs has shape (batch_size, input_dim),\n",
    "        #And kernel has shape (input_dim, num_kernels, dim_per_kernel),\n",
    "        #Then activation will have shape (batch_size, num_kernels, dim_per_kernel).\n",
    "        #Got from dot product.\n",
    "        activation = tf.tensordot(inputs, self.kernel, axes=[[1], [0]])\n",
    "        \n",
    "        #Expands dimensions to compare all batch samples.\n",
    "        expanded_act = tf.expand_dims(activation, 3)\n",
    "        transposed_act = tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)\n",
    "\n",
    "        diff = expanded_act - transposed_act #\n",
    "        abs_diff = tf.reduce_sum(tf.abs(diff), 2) #Computes the absolute differences between all pairs of samples.\n",
    "\n",
    "        #Apply mask to avoid comparing a sample with itself.\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        mask = 1.0 - tf.eye(batch_size) \n",
    "        #Creates an identity matrix (tf.eye(batch_size)), which has 1s on the diagonal and subtracts it from 1.0. Ensures that the diagonal is 0.\n",
    "        mask = tf.expand_dims(mask, 1)\n",
    "\n",
    "        #Expands dimensions to match abs_diff.\n",
    "        if mask.shape.ndims != abs_diff.shape.ndims:\n",
    "            mask = tf.reshape(mask, [-1, mask.shape[1], batch_size])\n",
    "\n",
    "        exp = tf.exp(-abs_diff) * mask\n",
    "        minibatch_features = tf.reduce_sum(exp, 2)\n",
    "\n",
    "        return tf.concat([inputs, minibatch_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final shape is (batch_size, input_dim + num_kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Discriminator model with financial constraints\n",
    "def build_discriminator(use_minibatch_discrimination=False):\n",
    "    inputs = layers.Input(shape=(config.lob_dim,))\n",
    "    x = layers.Dense(512, activation='relu')(inputs)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    \n",
    "    # Add Minibatch Discrimination layer\n",
    "    if use_minibatch_discrimination:\n",
    "        x = MinibatchDiscrimination(num_kernels=100, dim_per_kernel=5)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x) #ouput a rate. \n",
    "    \n",
    "    # Apply penalties using Lambda layers. penalize negative prices/quantities.\n",
    "    penalties = layers.Lambda(lambda x: tf.nn.softplus(-x))(inputs)\n",
    "\n",
    "    bid_prices = inputs[:, :10]\n",
    "    ask_prices = inputs[:, 20:30]\n",
    "    bid_diff = bid_prices[:, :-1] - bid_prices[:, 1:]\n",
    "    ask_diff = ask_prices[:, 1:] - ask_prices[:, :-1]\n",
    "    \n",
    "    # Fix: Padding to maintain shape consistency\n",
    "    #bid_diff_padded = layers.Lambda(lambda x: tf.pad(x, [[0, 0], [0, 1]]))(bid_diff)\n",
    "    #ask_diff_padded = layers.Lambda(lambda x: tf.pad(x, [[0, 0], [1, 0]]))(ask_diff)\n",
    "    #penalties = layers.Concatenate(axis=1)([penalties, bid_diff_padded, ask_diff_padded])\n",
    "    \n",
    "    max_bid = layers.Lambda(lambda x: tf.reduce_logsumexp(x, axis=1, keepdims=True))(bid_prices)\n",
    "    min_ask = layers.Lambda(lambda x: -tf.reduce_logsumexp(x, axis=1, keepdims=True))(ask_prices)\n",
    "    penalties += layers.Lambda(lambda x: tf.nn.softplus(x))(max_bid - min_ask)\n",
    "    \n",
    "    model = models.Model(inputs, [output, penalties])\n",
    "    \n",
    "    # Print the model output dimensions\n",
    "    print(f\"Discriminator Model Output Shapes:\")\n",
    "    print(f\" - Output (Real/Fake Probability): {output.shape}\")\n",
    "    print(f\" - Penalties (Financial Constraints): {penalties.shape}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define faulty rate computation\n",
    "def compute_faulty_rate(lob_tensor):\n",
    "    bid_prices = lob_tensor[:, :10]\n",
    "    ask_prices = lob_tensor[:, 20:30]\n",
    "    bid_quantities = lob_tensor[:, 10:20]\n",
    "    ask_quantities = lob_tensor[:, 30:40]\n",
    "\n",
    "    faulty_count = tf.reduce_sum(tf.cast(bid_prices[:, 0] >= ask_prices[:, 0], tf.float32))\n",
    "    faulty_count += tf.reduce_sum(tf.cast(bid_prices[:, :-1] <= bid_prices[:, 1:], tf.float32))\n",
    "    faulty_count += tf.reduce_sum(tf.cast(ask_prices[:, :-1] >= ask_prices[:, 1:], tf.float32))\n",
    "    faulty_count += tf.reduce_sum(tf.cast(bid_quantities < 0, tf.float32))\n",
    "    faulty_count += tf.reduce_sum(tf.cast(ask_quantities < 0, tf.float32))\n",
    "    \n",
    "    total_elements = tf.size(lob_tensor, out_type=tf.float32)\n",
    "    faulty_rate = faulty_count / total_elements\n",
    "    return faulty_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Model Output Shapes:\n",
      " - Generated LOB Data: (None, 40)\n",
      " - Penalty Score (Summed Constraints): (None, 60)\n",
      "Discriminator Model Output Shapes:\n",
      " - Output (Real/Fake Probability): (None, 1)\n",
      " - Penalties (Financial Constraints): (None, 40)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb74598bdf0f498fa386c17d5cfd7166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Loss D: 24.4118, Loss G: 12.9762\n",
      "Epoch 20/100 - Loss D: 24.8841, Loss G: 13.0898\n",
      "Epoch 30/100 - Loss D: 24.2593, Loss G: 12.9902\n",
      "Epoch 40/100 - Loss D: 24.6133, Loss G: 13.1201\n",
      "Epoch 50/100 - Loss D: 24.1242, Loss G: 12.9613\n",
      "Epoch 60/100 - Loss D: 25.0973, Loss G: 13.2964\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "optimizer_g = optimizers.Adam(learning_rate=config.learning_rate_g, beta_1=config.beta1, beta_2=config.beta2)\n",
    "optimizer_d = optimizers.Adam(learning_rate=config.learning_rate_d, beta_1=config.beta1, beta_2=config.beta2)\n",
    "\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "lambda_penalty = 2  # Adjust this weight based on importance of penalty term\n",
    "\n",
    "for epoch in tqdm(range(config.epochs), desc='Training Progress'):\n",
    "    for real_batch in lob_dataset:\n",
    "        batch_size = tf.shape(real_batch)[0]\n",
    "        real_labels = tf.ones((batch_size, 1)) * (1 - config.label_smoothing)\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        z = tf.random.normal((batch_size, config.z_dim))\n",
    "        fake_data, fake_penalty = generator(z)\n",
    "\n",
    "        # Train Discriminator\n",
    "        with tf.GradientTape() as tape_d:\n",
    "            real_pred, real_penalty = discriminator(real_batch)\n",
    "            fake_pred, fake_penalty = discriminator(fake_data)\n",
    "\n",
    "            # Binary cross-entropy loss\n",
    "            loss_d = bce_loss(real_labels, real_pred) + bce_loss(fake_labels, fake_pred)\n",
    "\n",
    "            # Add penalty term to discriminator loss\n",
    "            loss_d += lambda_penalty * (tf.reduce_mean(real_penalty) + tf.reduce_mean(fake_penalty))\n",
    "\n",
    "        grads_d = tape_d.gradient(loss_d, discriminator.trainable_variables)\n",
    "        optimizer_d.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
    "\n",
    "        # Train Generator\n",
    "        with tf.GradientTape() as tape_g:\n",
    "            fake_data, fake_penalty = generator(z)\n",
    "            fake_pred, fake_penalty = discriminator(fake_data)\n",
    "\n",
    "            # Generator loss (trying to fool the discriminator)\n",
    "            loss_g = bce_loss(real_labels, fake_pred)\n",
    "\n",
    "            # Add penalty term to generator loss\n",
    "            loss_g += lambda_penalty * tf.reduce_mean(fake_penalty)\n",
    "\n",
    "        grads_g = tape_g.gradient(loss_g, generator.trainable_variables)\n",
    "        optimizer_g.apply_gradients(zip(grads_g, generator.trainable_variables))\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs} - Loss D: {loss_d.numpy():.4f}, Loss G: {loss_g.numpy():.4f}\")\n",
    "\n",
    "# Generate synthetic LOB data\n",
    "z = tf.random.normal((10, config.z_dim))\n",
    "synthetic_lob, _ = generator(z)  # Extract only the generated LOB data\n",
    "synthetic_lob = synthetic_lob.numpy()\n",
    "synthetic_lob = scaler.inverse_transform(synthetic_lob)\n",
    "synthetic_lob_df = pd.DataFrame(synthetic_lob, columns=lob_features)\n",
    "\n",
    "\n",
    "# Compute and print faulty rate\n",
    "synthetic_lob_tensor = tf.convert_to_tensor(synthetic_lob, dtype=tf.float32)\n",
    "faulty_rate = compute_faulty_rate(synthetic_lob_tensor)\n",
    "print(\"Faulty Rate for Synthetic Data:\", faulty_rate.numpy())\n",
    "\n",
    "# Print synthetic LOB data\n",
    "print(\"Synthetic LOB Data:\")\n",
    "print(synthetic_lob_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Minibatch Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Model Output Shapes:\n",
      " - Generated LOB Data: (None, 40)\n",
      " - Penalty Score (Summed Constraints): (None, 60)\n",
      "Discriminator Model Output Shapes:\n",
      " - Output (Real/Fake Probability): (None, 1)\n",
      " - Penalties (Financial Constraints): (None, 40)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f82bf5cc68548aeabbf4fe48f5a826b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Loss D: 24.8818, Loss G: 24.1941\n",
      "Epoch 20/50 - Loss D: 26.3402, Loss G: 13.0394\n",
      "Epoch 30/50 - Loss D: 27.0428, Loss G: 13.5552\n",
      "Epoch 40/50 - Loss D: 26.1898, Loss G: 13.1127\n",
      "Epoch 50/50 - Loss D: 26.7893, Loss G: 13.5987\n",
      "Faulty Rate for Synthetic Data: 0.3925\n",
      "Synthetic LOB Data:\n",
      "            b0p           b1p           b2p           b3p           b4p  \\\n",
      "0  95498.570312  95498.867188  95533.718750  95521.492188  95529.054688   \n",
      "1  94607.898438  94590.335938  94608.367188  94615.851562  94602.132812   \n",
      "2  94665.531250  94611.617188  94627.156250  94616.945312  94606.460938   \n",
      "3  95487.187500  95492.117188  95525.140625  95516.234375  95522.695312   \n",
      "4  95467.515625  95474.546875  95508.531250  95501.242188  95514.187500   \n",
      "\n",
      "            b5p           b6p           b7p           b8p           b9p  ...  \\\n",
      "0  95523.531250  95506.046875  95529.859375  95530.460938  95498.953125  ...   \n",
      "1  94566.492188  94625.609375  94600.000000  94632.734375  94532.515625  ...   \n",
      "2  94591.570312  94631.531250  94626.812500  94616.601562  94595.125000  ...   \n",
      "3  95517.437500  95500.320312  95522.226562  95526.257812  95483.945312  ...   \n",
      "4  95497.187500  95490.625000  95504.265625  95506.351562  95476.773438  ...   \n",
      "\n",
      "         a0q       a1q       a2q       a3q       a4q       a5q       a6q  \\\n",
      "0   4.721679  0.471716 -0.120344 -0.436609  0.861140  0.079652  0.084843   \n",
      "1  22.155605 -0.071835 -1.305120 -1.272046  0.206371  0.062797 -0.019707   \n",
      "2  11.106270 -0.092785 -0.313254 -0.469664  0.180297  0.198327  0.289226   \n",
      "3  13.159040  0.424037 -0.018273 -0.392917  0.876867  0.052328  0.079206   \n",
      "4  10.027742  0.471669 -0.268726 -0.564257  0.703971  0.116761  0.133914   \n",
      "\n",
      "        a7q       a8q       a9q  \n",
      "0  0.595665 -1.556059  0.355366  \n",
      "1 -0.137225 -1.445157 -0.437901  \n",
      "2 -0.892063 -0.136088 -0.472513  \n",
      "3  0.922985 -0.679722  0.275487  \n",
      "4  0.703820 -0.533938  0.177706  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator()\n",
    "discriminator = build_discriminator(use_minibatch_discrimination=True)  \n",
    "\n",
    "optimizer_g = optimizers.Adam(learning_rate=config.learning_rate_g, beta_1=config.beta1, beta_2=config.beta2)\n",
    "optimizer_d = optimizers.Adam(learning_rate=config.learning_rate_d, beta_1=config.beta1, beta_2=config.beta2)\n",
    "\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "lambda_penalty = 2  # Adjust this weight based on importance of penalty term\n",
    "\n",
    "for epoch in tqdm(range(config.epochs), desc='Training Progress'):\n",
    "    for real_batch in lob_dataset:\n",
    "        batch_size = tf.shape(real_batch)[0]\n",
    "        real_labels = tf.ones((batch_size, 1)) * (1 - config.label_smoothing)\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        z = tf.random.normal((batch_size, config.z_dim))\n",
    "        fake_data, fake_penalty = generator(z)\n",
    "\n",
    "        # Train Discriminator\n",
    "        with tf.GradientTape() as tape_d:\n",
    "            real_pred, real_penalty = discriminator(real_batch)\n",
    "            fake_pred, fake_penalty = discriminator(fake_data)\n",
    "\n",
    "            # Binary cross-entropy loss\n",
    "            loss_d = bce_loss(real_labels, real_pred) + bce_loss(fake_labels, fake_pred)\n",
    "\n",
    "            # Add penalty term to discriminator loss\n",
    "            loss_d += lambda_penalty * (tf.reduce_mean(real_penalty) + tf.reduce_mean(fake_penalty))\n",
    "\n",
    "        grads_d = tape_d.gradient(loss_d, discriminator.trainable_variables)\n",
    "        optimizer_d.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
    "\n",
    "        # Train Generator\n",
    "        with tf.GradientTape() as tape_g:\n",
    "            fake_data, fake_penalty = generator(z)\n",
    "            fake_pred, fake_penalty = discriminator(fake_data)\n",
    "\n",
    "            # Generator loss (trying to fool the discriminator)\n",
    "            loss_g = bce_loss(real_labels, fake_pred)\n",
    "\n",
    "            # Add penalty term to generator loss\n",
    "            loss_g += lambda_penalty * tf.reduce_mean(fake_penalty)\n",
    "\n",
    "        grads_g = tape_g.gradient(loss_g, generator.trainable_variables)\n",
    "        optimizer_g.apply_gradients(zip(grads_g, generator.trainable_variables))\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs} - Loss D: {loss_d.numpy():.4f}, Loss G: {loss_g.numpy():.4f}\")\n",
    "\n",
    "# Generate synthetic LOB data\n",
    "z = tf.random.normal((10, config.z_dim))\n",
    "synthetic_lob, _ = generator(z)  # Extract only the generated LOB data\n",
    "synthetic_lob = synthetic_lob.numpy()\n",
    "synthetic_lob = scaler.inverse_transform(synthetic_lob)\n",
    "synthetic_lob_df = pd.DataFrame(synthetic_lob, columns=lob_features)\n",
    "\n",
    "\n",
    "# Compute and print faulty rate\n",
    "synthetic_lob_tensor = tf.convert_to_tensor(synthetic_lob, dtype=tf.float32)\n",
    "faulty_rate = compute_faulty_rate(synthetic_lob_tensor)\n",
    "print(\"Faulty Rate for Synthetic Data:\", faulty_rate.numpy())\n",
    "\n",
    "# Print synthetic LOB data\n",
    "print(\"Synthetic LOB Data:\")\n",
    "print(synthetic_lob_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow10CUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
