{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries and Setup Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from typing import Dict, List, Tuple, Optional, Union, Callable\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU Available: {len(gpus) > 0}\")\n",
    "if len(gpus) > 0:\n",
    "    print(f\"GPU Details: {gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for Improved TransGAN training and architecture\"\"\"\n",
    "    z_dim = 128                                    # Dimension of the random noise vector input to the generator\n",
    "    base_size = 4                                  # Initial spatial resolution (height and width) of the feature map \n",
    "    learning_rate_g = 1e-4                         # Learning rate for the discriminator optimizer\n",
    "    learning_rate_d = 1e-4                         # Learning rate for the discriminator optimizer\n",
    "    beta1 = 0.5                                    # First momentum parameter for Adam optimizer\n",
    "    beta2 = 0.999                                  # Second momentum parameter for Adam optimizer\n",
    "    generator_steps = 1                            # Number of generator updates per iteration\n",
    "    use_feature_matching = False                   # Whether to use feature matching loss\n",
    "    use_historical_averaging = False               # Whether to use historical averaging of parameters\n",
    "    use_minibatch_discrimination = False           # Whether to use minibatch discrimination\n",
    "    feature_matching_weight = 1.0                  # Weight for the feature matching loss\n",
    "    historical_averaging_weight = 0.1              # Weight for historical averaging penalty\n",
    "    label_smoothing = 0.0                          # Amount of label smoothing for discriminator\n",
    "    generator_target_prob = 0.9                    # Target probability for generator labels\n",
    "    sample_freq = 1                                # Frequency of generating sample images during training\n",
    "    save_freq = 10                                 # Frequency of saving model checkpoints\n",
    "    checkpoint_dir = \"./checkpoints\"               # Directory to save model checkpoints\n",
    "    sample_dir = \"./samples\"                       # Directory to save generated samples\n",
    "    log_dir = \"./logs\"                             # Directory to save training logs\n",
    "    batch_size = 64                                # Number of samples processed in each training step\n",
    "\n",
    "### NOT shown in the config class: image_size: int = 32, dataset: str = 'cifar10'\n",
    "\n",
    "@classmethod\n",
    "def from_json(cls, json_path):\n",
    "    \"\"\"Load configuration from a JSON file\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "    return cls(**config_dict)\n",
    "\n",
    "def to_json(self, json_path):\n",
    "    \"\"\"Save configuration to a JSON file\"\"\"\n",
    "    config_dict = {k: v for k, v in self.__dict__.items()}\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "def __str__(self):\n",
    "    \"\"\"String representation of the configuration\"\"\"\n",
    "    return '\\n'.join(f\"{k}: {v}\" for k, v in self.__dict__.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GANConfig:\n",
    "    \"\"\"Configuration for GAN training and architecture\"\"\"\n",
    "    batch_size: int = 64                      # Number of samples processed in each training step\n",
    "    z_dim: int = 100                          # Dimension of the random noise vector input to the generator\n",
    "    learning_rate_d: float = 0.0002           # Learning rate for the discriminator optimizer\n",
    "    learning_rate_g: float = 0.0002           # Learning rate for the generator optimizer\n",
    "    beta1: float = 0.5                        # First momentum parameter for Adam optimizer\n",
    "    beta2: float = 0.999                      # Second momentum parameter for Adam optimizer\n",
    "    image_size: int = 32                      # Size of the generated images in pixels (32×32)\n",
    "    channels: int = 3                         # Number of color channels in the images (RGB)\n",
    "    use_feature_matching: bool = True         # Whether to use feature matching loss\n",
    "    use_minibatch_discrimination: bool = True # Whether to use minibatch discrimination\n",
    "    use_historical_averaging: bool = True     # Whether to use historical averaging of parameters\n",
    "    use_virtual_batch_norm: bool = False      # Whether to use virtual batch normalization\n",
    "    feature_matching_weight: float = 1.0      # Weight for the feature matching loss\n",
    "    historical_averaging_weight: float = 0.0001 # Weight for historical averaging penalty\n",
    "    label_smoothing: float = 0.1              # Amount of label smoothing for discriminator\n",
    "    generator_target_prob: float = 0.65       # Target probability for generator labels\n",
    "    dataset: str = 'cifar10'                  # Dataset to use for training\n",
    "    epochs: int = 30                          # Number of training epochs\n",
    "    save_freq: int = 5                        # Frequency of saving model checkpoints\n",
    "    sample_freq: int = 1                      # Frequency of generating sample images\n",
    "    log_freq: int = 5                         # Frequency of logging training metrics\n",
    "    sample_size: int = 36                     # Number of samples to generate\n",
    "    use_seed: int = 42                        # Random seed for reproducibility\n",
    "    checkpoint_dir: str = 'checkpoints'       # Directory to save model checkpoints\n",
    "    sample_dir: str = 'samples'               # Directory to save generated samples\n",
    "    log_dir: str = 'logs'                     # Directory to save training logs\n",
    "    generator_steps: int = 1                  # Number of generator updates per iteration\n",
    "    discriminator_steps: int = 1              # Number of discriminator updates per iteration\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_path):\n",
    "        \"\"\"Load configuration from a JSON file\"\"\"\n",
    "        with open(json_path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        return cls(**config_dict)\n",
    "\n",
    "    def to_json(self, json_path):\n",
    "        \"\"\"Save configuration to a JSON file\"\"\"\n",
    "        config_dict = {k: v for k, v in self.__dict__.items()}\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the configuration\"\"\"\n",
    "        return '\\n'.join(f\"{k}: {v}\" for k, v in self.__dict__.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('samples', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing the Core TransGAN Architectures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualBatchNormalization(layers.Layer):\n",
    "    \"\"\"Virtual Batch Normalization Layer\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, **kwargs):\n",
    "        super(VirtualBatchNormalization, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        self.reference_batch_set = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.ndim = len(input_shape)\n",
    "        shape = [1] * self.ndim\n",
    "        shape[-1] = input_shape[-1]\n",
    "\n",
    "        self.gamma = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=tf.random_normal_initializer(1.0, 0.02),\n",
    "            name='gamma',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            name='beta',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.ref_mean = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            name='ref_mean',\n",
    "            trainable=False\n",
    "        )\n",
    "        self.ref_var = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer=tf.ones_initializer(),\n",
    "            name='ref_var',\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "        super(VirtualBatchNormalization, self).build(input_shape)\n",
    "\n",
    "    def _get_axis(self):\n",
    "        return list(range(self.ndim - 1))\n",
    "\n",
    "    def set_reference_batch(self, x):\n",
    "        axes = self._get_axis()\n",
    "        mean = tf.reduce_mean(x, axis=axes, keepdims=True)\n",
    "        var = tf.reduce_mean(tf.square(x - mean), axis=axes, keepdims=True)\n",
    "\n",
    "        self.ref_mean.assign(mean)\n",
    "        self.ref_var.assign(var)\n",
    "        self.reference_batch_set = True\n",
    "\n",
    "    def call(self, inputs, set_reference=False, **kwargs):\n",
    "        if set_reference or not self.reference_batch_set:\n",
    "            axes = self._get_axis()\n",
    "            ref_mean = tf.reduce_mean(inputs, axis=axes, keepdims=True)\n",
    "            ref_var = tf.reduce_mean(tf.square(inputs - ref_mean), axis=axes, keepdims=True)\n",
    "\n",
    "            self.ref_mean.assign(ref_mean)\n",
    "            self.ref_var.assign(ref_var)\n",
    "            self.reference_batch_set = True\n",
    "\n",
    "            batch_mean = ref_mean\n",
    "            batch_var = ref_var\n",
    "        else:\n",
    "            axes = self._get_axis()\n",
    "            batch_mean = tf.reduce_mean(inputs, axis=axes, keepdims=True)\n",
    "            batch_var = tf.reduce_mean(tf.square(inputs - batch_mean), axis=axes, keepdims=True)\n",
    "\n",
    "            batch_mean = 0.5 * (batch_mean + self.ref_mean)\n",
    "            batch_var = 0.5 * (batch_var + self.ref_var)\n",
    "\n",
    "        batch_var = tf.maximum(batch_var, self.epsilon)\n",
    "\n",
    "        x_norm = (inputs - batch_mean) / tf.sqrt(batch_var)\n",
    "\n",
    "        gamma_reshaped = self.gamma\n",
    "        beta_reshaped = self.beta\n",
    "\n",
    "        if self.ndim > 2:\n",
    "            gamma_reshaped = tf.reshape(self.gamma, [1] * (self.ndim - 1) + [self.gamma.shape[0]])\n",
    "            beta_reshaped = tf.reshape(self.beta, [1] * (self.ndim - 1) + [self.beta.shape[0]])\n",
    "\n",
    "        return x_norm * gamma_reshaped + beta_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EmbeddingProjection Layer**: \n",
    "\n",
    "The EmbeddingProjection layer is designed to project random noise (latent vectors) into a structured feature space, transforming a 1D vector into a spatial feature map. This transformation is a crucial step in the generator of TransGAN, where convolutional layers are replaced with transformer-based architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingProjection(tf.keras.Model):\n",
    "    \"\"\"Embedding Projection Layer\"\"\"\n",
    "    def __init__(self, in_features, out_features, base_size):\n",
    "        super(EmbeddingProjection, self).__init__()\n",
    "        self.in_features = in_features  \n",
    "        self.base_size = base_size # Initial spatial resolution (height and width) of the feature map\n",
    "        self.out_features = out_features\n",
    "        self.fc = tf.keras.layers.Dense((self.base_size ** 2) * self.out_features, input_shape=(in_features,))\n",
    "\n",
    "    def call(self, latents):\n",
    "        flattened = self.fc(latents)\n",
    "        return tf.reshape(flattened, (-1, self.base_size ** 2, self.out_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GridAttention Layer**\n",
    "\n",
    "The GridAttention layer is a self-attention mechanism designed for TransGAN. It follows the multi-head self-attention (MHSA) mechanism, where the input to this layer is a set of patch embeddings, then transformed into query ($Q$), key ($K$), and value ($V$) tensors, and the output is an updated set of embeddings after applying attention which is computed using dot-product interactions. By applying attention within structured feature maps, this layer enables spatially-aware self-attention, improving information flow and enabling the generator to capture local and global dependencies efficiently. Compared with normal multi-head self-attention, the grid attention mechanism limits the calculations to local regions to reduce the computational cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridAttention(Model):\n",
    "    \"\"\"Grid Attention Layer\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, window_size, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., noise_enabled=False):\n",
    "        super(GridAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scale = qk_scale if qk_scale is not None else head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        self.noise_enabled = noise_enabled\n",
    "\n",
    "        self.qkv = layers.Dense(embed_dim * 3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(attn_drop)\n",
    "        self.proj = layers.Dense(embed_dim)\n",
    "        self.proj_drop = layers.Dropout(proj_drop)\n",
    "\n",
    "        if self.noise_enabled:\n",
    "            self.noise_strength = self.add_weight(shape=(), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "        if self.window_size:\n",
    "            coords_h = tf.range(window_size)\n",
    "            coords_w = tf.range(window_size)\n",
    "            coords = tf.stack(tf.meshgrid(coords_h, coords_w, indexing='ij'))  # shape: (2, Wh, Ww)\n",
    "            coords_flatten = tf.reshape(coords, (2, -1))  # shape: (2, Wh*Ww)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # shape: (2, Wh*Ww, Wh*Ww)\n",
    "            relative_coords = tf.transpose(relative_coords, (1, 2, 0))  # shape: (Wh*Ww, Wh*Ww, 2)\n",
    "            relative_coords = tf.cast(relative_coords, dtype=tf.int32)\n",
    "            relative_coords = relative_coords + (window_size - 1)\n",
    "            relative_coords = relative_coords[:, :, 0] * (2 * window_size - 1) + relative_coords[:, :, 1]\n",
    "\n",
    "            self.relative_position_index = tf.Variable(relative_coords, trainable=False, dtype=tf.int32)\n",
    "\n",
    "            bias_table_shape = ((2 * window_size - 1) * (2 * window_size - 1), num_heads)\n",
    "            self.relative_position_bias_table = self.add_weight(shape=bias_table_shape, initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02), trainable=True)\n",
    "\n",
    "    def call(self, patch_embeddings, training=False):\n",
    "        batch_size, n_tokens, embedding_dim = tf.shape(patch_embeddings)[0], tf.shape(patch_embeddings)[1], tf.shape(patch_embeddings)[2]\n",
    "\n",
    "        if self.noise_enabled:\n",
    "            noise = tf.random.normal((batch_size, n_tokens, 1)) * self.noise_strength\n",
    "            patch_embeddings = patch_embeddings + noise\n",
    "\n",
    "        qkv = self.qkv(patch_embeddings)  # (batch_size, n_tokens, 3 * embedding_dim)\n",
    "        qkv = tf.reshape(qkv, (batch_size, n_tokens, 3, self.num_heads, embedding_dim // self.num_heads))\n",
    "        qkv = tf.transpose(qkv, (2, 0, 3, 1, 4))  # (3, batch_size, num_heads, n_tokens, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "        if self.window_size:\n",
    "            relative_position_bias_index = tf.reshape(self.relative_position_index, [-1])\n",
    "            relative_position_bias = tf.gather(self.relative_position_bias_table, relative_position_bias_index)\n",
    "            relative_position_bias = tf.reshape(relative_position_bias, (self.window_size * self.window_size, self.window_size * self.window_size, -1))\n",
    "            relative_position_bias = tf.transpose(relative_position_bias, (2, 0, 1))  # (num_heads, Wh*Ww, Wh*Ww)\n",
    "            attn = attn + tf.expand_dims(relative_position_bias, axis=0)  # Broadcast across batch_size\n",
    "\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "\n",
    "        aggregated = tf.matmul(attn, v)\n",
    "        aggregated = tf.transpose(aggregated, (0, 2, 1, 3))  # (batch_size, n_tokens, num_heads, head_dim)\n",
    "        aggregated = tf.reshape(aggregated, (batch_size, n_tokens, embedding_dim))\n",
    "\n",
    "        aggregated = self.proj(aggregated)\n",
    "        aggregated = self.proj_drop(aggregated, training=training)\n",
    "\n",
    "        return aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridAttention Layer above works by:\n",
    "\n",
    "1. Projects a sequence of patch embeddings (input)  into Query ($Q$), Key ($K$), and Value ($V$) tensors using a fully connected layer (Dense) to compute $Q$, $K$, and $V$ simultaneously.\n",
    "\n",
    "2. Computes attention scores using scaled dot-product attention\n",
    "   \n",
    "   2.1 Performs matrix multiplication between $Q$ and $K$ to compute raw attention scores.\n",
    "\n",
    "   2.2 Incorporates relative positional bias (if window_size is specified).\n",
    "\n",
    "3. Retrieves learnable positional biases and adds them to the attention scores\n",
    "\n",
    "4. Applies softmax normalization to obtain attention weights and uses attention weights to aggregate values across all patches.\n",
    "\n",
    "5. Applies final projection and dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EncoderBlock Layer**\n",
    "\n",
    "The EncoderBlock is a transformer-based processing unit designed to refine patch embeddings using self-attention and feedforward layers. It operates as a basic building block for deep vision models, like GridAttention. By applying attention-based refinement and feedforward transformations, this layer helps encode spatial and contextual relationships, making it a crucial component for deep vision transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(Model):\n",
    "    \"\"\"Transformer Encoder Block\"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_feedforward, window_size, dropout_rate=0, activation=None, noise_enabled=False):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        # Initialize the attention mechanism  \n",
    "        self.attention = GridAttention(d_model, n_heads, window_size, noise_enabled=noise_enabled)\n",
    "        \n",
    "        # First dense layer for the feedforward network\n",
    "        self.feedforward_dense1 = layers.Dense(d_feedforward)\n",
    "        # Set the activation function, default is GELU (Gaussian Error Linear Unit) \n",
    "        self.activation = activation if callable(activation) else tf.nn.gelu ####  GELU\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        # Second dense layer for the feedforward network\n",
    "        self.feedforward_dense2 = layers.Dense(d_model)\n",
    "        \n",
    "        # Layer normalization to stabilize the learning  \n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, patch_embeddings, training=False):\n",
    "        # Perform LayerNorm before calculation of attention\n",
    "        attn_out = self.attention(self.norm1(patch_embeddings), training=training)\n",
    "        patch_embeddings = patch_embeddings + attn_out  # Residual connection \n",
    "\n",
    "        # Normalize and then apply the feedforward network  \n",
    "        ff_out = self.feedforward_dense1(self.norm2(patch_embeddings))\n",
    "        ff_out = self.activation(ff_out)\n",
    "        ff_out = self.dropout1(ff_out, training=training)\n",
    "        ff_out = self.feedforward_dense2(ff_out)\n",
    "\n",
    "        patch_embeddings = patch_embeddings + ff_out  \n",
    "        return patch_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer is structured similarly to a Transformer encoder:\n",
    "\n",
    "1. First, applies self-attention to model relationships between patches.\n",
    "\n",
    "2. Second, passes the embeddings through a feedforward network (MLP) for additional transformation.\n",
    "\n",
    "3. Third, incorporates Layer Normalization (LayerNorm) and residual connections, ensuring stable gradient flow and improved convergence.\n",
    "\n",
    "4. Forth, Optional dropout and activation functions provide regularization and non-linearity, enhancing generalization.\n",
    "\n",
    "The input is a set of patch embeddings, and the output is an updated set of embeddings that have been refined through attention and feedforward transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StageBlock Layer**\n",
    "\n",
    "The StageBlock serves as a higher-level processing unit in a transformer-based architecture. It is composed of multiple EncoderBlock layers, stacked together to progressively refine patch embeddings. Each EncoderBlock in the StageBlock applies self-attention and feedforward transformations, helping the model capture both local and global dependencies. By stacking multiple encoder blocks, StageBlock allows deeper feature extraction and hierarchical representation learning.\n",
    "\n",
    "The input is a set of patch embeddings, and the output is a more refined set of embeddings after passing through multiple EncoderBlock layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageBlock(Model):\n",
    "    \"\"\"Transformer Stage Block\"\"\"\n",
    "    def __init__(self, depth, num_heads, d_embeddings, d_ratio, window_size, activation=None, noise_enabled=False):\n",
    "        super(StageBlock, self).__init__()\n",
    "\n",
    "        # Create a list of EncoderBlock instances based on the specified depth\n",
    "        self.blocks = [\n",
    "            EncoderBlock(\n",
    "                d_model=d_embeddings,\n",
    "                n_heads=num_heads,\n",
    "                d_feedforward=d_embeddings * d_ratio,\n",
    "                window_size=window_size,\n",
    "                activation=activation,\n",
    "                noise_enabled=noise_enabled\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ]\n",
    "\n",
    "    def call(self, patch_embeddings, training=False):\n",
    "        # Execute each EncoderBlock sequentially\n",
    "        for block in self.blocks:\n",
    "            patch_embeddings = block(patch_embeddings, training=training) \n",
    "        return patch_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StageBlock Lay works by:\n",
    "\n",
    "1. Receives a sequence of patch embeddings (patch_embeddings) as input\n",
    "\n",
    "2. Initializes multiple EncoderBlock layers\n",
    "   \n",
    "   2.1 Each block consists of: (1) Multi-head self-attention (via GridAttention). (2) Feedforward transformations (MLP). (3) Residual connections and normalization for stable training.\n",
    "\n",
    "3. Sequentially processes embeddings through multiple encoder blocks\n",
    "\n",
    "4. Outputs the final refined embeddings which retain hierarchical and contextual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_shuffle(input_tensor, scale_factor):\n",
    "    # Get the shape of the input tensor \n",
    "    batch_size, height, width, channels = input_tensor.shape\n",
    "\n",
    "    # Ensure the number of channels is divisible by scale_factor squared \n",
    "    if channels % (scale_factor ** 2) != 0:\n",
    "        raise ValueError(\"Channels must be divisible by scale_factor^2.\")\n",
    "\n",
    "    # Calculate the new number of channels  \n",
    "    new_channels = channels // (scale_factor ** 2)\n",
    "\n",
    "    # Reshape the tensor to (N, H, W, r, r, C)  \n",
    "    reshaped_tensor = tf.reshape(input_tensor, (batch_size, height, width, scale_factor, scale_factor, new_channels))\n",
    "\n",
    "    # Transpose to (N, H, r, W, r, C)  \n",
    "    transposed_tensor = tf.transpose(reshaped_tensor, [0, 1, 3, 2, 4, 5])\n",
    "\n",
    "    # Final shape will be (N, H * r, W * r, C) \n",
    "    output_tensor = tf.reshape(transposed_tensor, (batch_size, height * scale_factor, width * scale_factor, new_channels))\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resampling Layer**\n",
    "\n",
    "The Resampling layer is designed to adjust the spatial resolution of feature maps in TransGAN. It can perform both upsampling and downsampling, depending on the scale_factor. By dynamically adjusting feature map resolution, this layer allows the generator to process multi-scale representations effectively, ensuring smooth feature transitions during upsampling and downsampling.\n",
    "\n",
    "If scale_factor > 1, it upsamples the feature map using Pixel Shuffle, which redistributes channel data into spatial dimensions.\n",
    "\n",
    "If scale_factor < 1, it downsamples the feature map using Average Pooling, reducing spatial resolution while preserving information.\n",
    "\n",
    "This layer ensures that the generator maintains spatial consistency as features are refined and upsampled to higher resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resampling(layers.Layer):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(Resampling, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.is_upsampling = scale_factor > 1\n",
    "        \n",
    "        # If not upsampling, set up Average Pooling for downsampling \n",
    "        if not self.is_upsampling:\n",
    "            self.resampling = layers.AveragePooling2D(pool_size=int(1 / scale_factor))  # 下采样\n",
    "\n",
    "    def pixel_shuffle(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Pixel Shuffle operation, rearranges channel data into spatial dimensions.\n",
    "        \"\"\"\n",
    "        batch_size, height, width, channels = tf.shape(input_tensor)[0], tf.shape(input_tensor)[1], \\\n",
    "                                              tf.shape(input_tensor)[2], tf.shape(input_tensor)[3]\n",
    "\n",
    "        # Ensure channels are a multiple of scale_factor squared  \n",
    "        new_channels = channels // (self.scale_factor ** 2)\n",
    "\n",
    "        reshaped_tensor = tf.reshape(input_tensor, (batch_size, height, width, self.scale_factor, self.scale_factor, new_channels))\n",
    "        transposed_tensor = tf.transpose(reshaped_tensor, [0, 1, 3, 2, 4, 5])\n",
    "        output_tensor = tf.reshape(transposed_tensor, (batch_size, height * self.scale_factor, width * self.scale_factor, new_channels))\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "    def call(self, embeddings, size=None):\n",
    "        \"\"\"\n",
    "        Upsamples (Pixel Shuffle) or downsamples (Average Pooling) the input embeddings.  \n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(embeddings)[0]\n",
    "        embedding_dim = embeddings.shape[-1]  # ✅ 直接使用 `shape`，避免 `tf.Tensor` 计算错误\n",
    "        size = tf.get_static_value(size) if isinstance(size, tf.Tensor) else size  # ✅ 解析 Tensor 为 Python `int`\n",
    "\n",
    "        # Ensure embedding_dim is divisible by scale_factor squared \n",
    "        tf.debugging.assert_equal(\n",
    "            embedding_dim % (self.scale_factor ** 2),\n",
    "            0,\n",
    "            message=f\"Embedding dim {embedding_dim} must be a multiple of scale_factor^2 ({self.scale_factor ** 2})!\"\n",
    "        )\n",
    "\n",
    "        reduced_embedding_dim = embedding_dim // (self.scale_factor ** 2)\n",
    "\n",
    "        # Reshape to (batch, size, size, embedding_dim)  \n",
    "        feature_maps = tf.reshape(embeddings, (batch_size, size, size, embedding_dim))\n",
    "\n",
    "        if self.is_upsampling:\n",
    "            # Perform Pixel Shuffle for upsampling  \n",
    "            resampled = self.pixel_shuffle(feature_maps)\n",
    "            new_size = size * self.scale_factor\n",
    "        else:\n",
    "            # Perform Average Pooling for downsampling and reshape accordingly \n",
    "            resampled = self.resampling(tf.transpose(feature_maps, (0, 3, 1, 2)))  # 转换到 NCHW 格式\n",
    "            resampled = tf.transpose(resampled, (0, 2, 3, 1))  # 转换回 NHWC 格式\n",
    "            new_size = size // self.scale_factor\n",
    "\n",
    "        # Ensure embedding dimension is correct after resampling\n",
    "        n_tokens_new = new_size * new_size\n",
    "        feature_maps = tf.reshape(resampled, (batch_size, n_tokens_new, reduced_embedding_dim))\n",
    "\n",
    "        return tf.transpose(feature_maps, (0, 1, 2)), new_size  # 返回 (batch, n_tokens_new, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling Lay works by:\n",
    "\n",
    "1. Receives a set of patch embeddings and a scale_factor as input and Determines whether to upsample or downsample\n",
    "   \n",
    "   1.1 If scale_factor > 1, $Pixel Shuffle$ function is used to increase resolution which can converts the channel dimension into spatial resolution. Then the output resolution increases from ($H$, $W$) to ($H$ * $scale\\_ factor$, $W$ * $scale\\_ factor$)\n",
    "   \n",
    "   1.2 If scale_factor < 1, $Average Pooling$ is used to reduce resolution which can reduce resolution from ($H$, $W$) to ($H$ / $scale_factor$, $W$ / $scale_factor$). Besides, layers.AveragePooling2D will be used to preserve feature integrity while reducing size\n",
    "\n",
    "2. Reshapes the output to maintain token structure.\n",
    "   \n",
    "   2.1 The spatially adjusted feature map is reshaped back into a structured token sequence.\n",
    "\n",
    "   2.2 The number of tokens updates based on the new spatial size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generator**\n",
    "\n",
    "The Generator model is designed as a transformer-based generator for image synthesis, following the structure of TransGAN. Unlike traditional CNN-based GANs, this generator leverages self-attention mechanisms instead of convolutional layers to progressively refine feature maps. By leveraging transformers, progressive upsampling, and spatial self-attention, this generator enables high-quality image synthesis while preserving long-range dependencies and global coherence. \n",
    "\n",
    "The key components of this generator include:\n",
    "\n",
    "1. Embedding Projection: Maps a random latent vector to an initial spatial feature map.\n",
    "\n",
    "2. Virtual Batch Normalization (VBN): Stabilizes feature distributions across batches.\n",
    "\n",
    "3. Positional Encoding: Provides spatial awareness to the self-attention mechanism.\n",
    "\n",
    "3. Transformer Encoder Blocks: Captures global dependencies and refines feature representations.\n",
    "\n",
    "4. Resampling Layer: Upsamples feature maps progressively to increase resolution.\n",
    "\n",
    "5. Output Projection: Uses a Conv2D layer to generate the final image, applying a tanh activation to normalize pixel values.\n",
    "\n",
    "This generator takes a random latent vector as input and outputs a synthetic image with values in the range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self, base_size=8, n_colors=3, embed_dim=1024, depths=(5, 4, 2), num_heads=4, ff_ratio=4, latent_dim=256):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.base_size = base_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Embedding layer: Projects the noise vector into feature space\n",
    "        self.embedding_projection = EmbeddingProjection(latent_dim, embed_dim, base_size)\n",
    "        self.vbn = VirtualBatchNormalization()  # Include Virtual Batch Normalization\n",
    "\n",
    "        self.n_stages = len(depths)\n",
    "\n",
    "        # Positional embeddings: Provide spatial context to embeddings\n",
    "        self.positional_embeddings = [\n",
    "            self.add_weight(\n",
    "                shape=(1, (self.base_size * (2 ** index)) ** 2, embed_dim // (4 ** index)),\n",
    "                initializer=tf.keras.initializers.RandomNormal(stddev=0.02),\n",
    "                trainable=True,\n",
    "                name=f'pos_embedding_{index}'\n",
    "            ) for index in range(self.n_stages)\n",
    "        ]\n",
    "\n",
    "        # Upsampling layer  \n",
    "        self.upsampling = Resampling(scale_factor=2)\n",
    "\n",
    "        # Transformer\n",
    "        self.stages = [\n",
    "            StageBlock(\n",
    "                depth,\n",
    "                num_heads,\n",
    "                d_ratio=ff_ratio,\n",
    "                window_size=base_size * (2 ** index),\n",
    "                d_embeddings=embed_dim // (4 ** index),\n",
    "            ) for index, depth in enumerate(depths)\n",
    "        ]\n",
    "\n",
    "        # Output projection: Convolution layer + tanh activation  \n",
    "        self.output_projection = layers.Conv2D(n_colors, kernel_size=1, activation=\"tanh\", data_format=\"channels_last\")\n",
    "\n",
    "        # Initialize reference batch\n",
    "        self.reference_batch = None\n",
    "\n",
    "    def call(self, latents, training=False, set_reference=False):\n",
    "\n",
    "        # 1. Initial embedding from latent vectors\n",
    "        patch_embeddings = self.embedding_projection(latents, training=training)\n",
    "        size = self.base_size\n",
    "\n",
    "        # 2. Set reference batch (used only at the start of training) \n",
    "        if set_reference or self.reference_batch is None:\n",
    "            self.reference_batch = patch_embeddings\n",
    "\n",
    "        # 3. Normalize using Virtual Batch Normalization \n",
    "        patch_embeddings = self.vbn(patch_embeddings, set_reference=set_reference)\n",
    "\n",
    "        print(f\"Initial patch_embeddings shape: {patch_embeddings.shape}\")\n",
    "\n",
    "        # 4. Pass through Transformer encoding blocks sequentially \n",
    "        for index, stage in enumerate(self.stages):\n",
    "            patch_embeddings += self.positional_embeddings[index]  # Add positional encoding  \n",
    "            patch_embeddings = stage(patch_embeddings, training=training)\n",
    "\n",
    "            # 5. Upsample (only execute in the first few layers) \n",
    "            if index < self.n_stages - 1:\n",
    "                patch_embeddings, size = self.upsampling(patch_embeddings, size=size)\n",
    "                print(f\"After Resampling stage {index}: {patch_embeddings.shape}, size: {size}\")\n",
    "\n",
    "        # 6. Reshape to Feature Map (N, H, W, C)  \n",
    "        batch_size = tf.shape(patch_embeddings)[0]\n",
    "        feature_maps = tf.reshape(patch_embeddings, (batch_size, size, size, -1))\n",
    "        print(f\"Feature maps before output projection: {feature_maps.shape}\")\n",
    "\n",
    "        # 7. Project through Conv2D to ensure output range is [-1, 1] \n",
    "        output = self.output_projection(feature_maps)\n",
    "        output = tf.clip_by_value(output, -1.0, 1.0)\n",
    "        print(f\"Output shape after projection: {output.shape}\")\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchDiscrimination(layers.Layer):\n",
    "    \"\"\"Minibatch Discrimination layer to prevent mode collapse\"\"\"\n",
    "    def __init__(self, num_kernels=100, dim_per_kernel=5):\n",
    "        super(MinibatchDiscrimination, self).__init__()\n",
    "        self.num_kernels = num_kernels  # Number of kernels for feature extraction\n",
    "        self.dim_per_kernel = dim_per_kernel  # Dimensionality of features per kernel\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Initialize weight matrix for the layer\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_shape[-1], self.num_kernels * self.dim_per_kernel),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Compute Minibatch features\"\"\"\n",
    "        x = tf.matmul(inputs, self.W)  # Matrix multiplication (batch, num_kernels * dim_per_kernel)\n",
    "        x = tf.reshape(x, (-1, self.num_kernels, self.dim_per_kernel))\n",
    "\n",
    "        # Calculate L1 distance between feature vectors  \n",
    "        diff = tf.expand_dims(x, axis=3) - tf.expand_dims(tf.transpose(x, [1, 2, 0]), axis=0)\n",
    "        abs_diff = tf.reduce_sum(tf.abs(diff), axis=2)\n",
    "\n",
    "        # Compute Minibatch features based on the distances  \n",
    "        minibatch_features = tf.reduce_sum(tf.exp(-abs_diff), axis=2)\n",
    "        \n",
    "        # Concatenate original inputs with the minibatch features  \n",
    "        return tf.concat([inputs, minibatch_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TransformerBlock**\n",
    "\n",
    "The TransformerBlock is an encoder module inspired by the Transformer architecture, primarily used in Vision Transformers (ViTs), TransGAN, and NLP models. It processes input embeddings through self-attention and feedforward layers, refining feature representations while maintaining global contextual dependencies. By leveraging self-attention, feedforward transformations, and residual connections, the TransformerBlock allows the model to learn complex feature relationships, making it an essential building block for vision transformers, TransGAN, and other deep learning architectures. The TransformerBlock is crucial for enabling the generator or other transformer-based architectures to effectively model long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\" Transformer Encoder Block\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim=512, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # 1. Multi-head self-attention mechanism\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.norm1 = layers.LayerNormalization()   # Layer normalization for the attention output \n",
    "        self.dropout1 = layers.Dropout(dropout_rate)   # Dropout for regularization\n",
    "\n",
    "        # 2. Feedforward neural network (MLP) \n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"gelu\"),  # First dense layer with GeLU activation \n",
    "            layers.Dense(embed_dim)  # Second dense layer to project back to the embedding dimension\n",
    "        ])\n",
    "        self.norm2 = layers.LayerNormalization()   # Layer normalization for MLP output\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)  # Dropout for the feedforward output\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        attn_output = self.attention(inputs, inputs)  # Self-attention mechanism \n",
    "        attn_output = self.dropout1(attn_output)  # Apply dropout to attention output\n",
    "        out1 = self.norm1(inputs + attn_output)  # Residual connection + normalization\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # Pass through feedforward network (MLP)\n",
    "        ffn_output = self.dropout2(ffn_output)  # Apply dropout to MLP output\n",
    "        return self.norm2(out1 + ffn_output)  # Residual connection + normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block contains:\n",
    "\n",
    "1. Multi-Head Self-Attention (MHA): Captures relationships between input tokens.\n",
    "\n",
    "2. Feedforward Network (FFN): Enhances feature representations after attention.\n",
    "\n",
    "3. Layer Normalization (LayerNorm): Stabilizes training by normalizing activations.\n",
    "\n",
    "4. Residual Connections: Helps retain original information and improve gradient flow.\n",
    "\n",
    "5. Dropout Regularization: Prevents overfitting by adding noise during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TransformerDiscriminator Layer**\n",
    "\n",
    "The TransformerDiscriminator is a transformer-based discriminator designed to distinguish between real and generated images. Unlike traditional CNN-based discriminators, this model processes images using self-attention mechanisms instead of convolutional layers, allowing it to capture global dependencies and long-range interactions. By leveraging self-attention, transformer-based processing, and minibatch discrimination, the TransformerDiscriminator provides a powerful alternative to traditional CNN-based discriminators, making it highly effective in GAN architectures like TransGAN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This discriminator follows a Vision Transformer (ViT)-like structure, consisting of:\n",
    "\n",
    "1. Patch Embedding Layer: Converts input images into a sequence of patch tokens.\n",
    "\n",
    "2. Transformer Encoder Blocks: Processes the patch embeddings using self-attention.\n",
    "\n",
    "3. Minibatch Discrimination (optional): Helps prevent mode collapse by introducing diversity-sensitive features.\n",
    "\n",
    "4. Normalization & MLP Classification Head: Outputs a real/fake probability score using a sigmoid activation function.\n",
    "\n",
    "The model takes an image as input and outputs a single probability indicating whether the input is real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDiscriminator(tf.keras.Model):\n",
    "    \"\"\" Transformer-based Discriminator \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, embed_dim=512, num_heads=4, depth=4, patch_size=4, use_minibatch=True):\n",
    "        super(TransformerDiscriminator, self).__init__()\n",
    "\n",
    "        self.img_size = img_size   # Size of the input image 32× 32\n",
    "        self.patch_size = patch_size  # Size of each patch\n",
    "        self.use_minibatch = use_minibatch  # Indicates whether to use minibatch discrimination\n",
    "\n",
    "        # 1️. Patch Embedding layer: divides the image into patches and projects them into a high-dimensional space\n",
    "        self.patch_embedding = layers.Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size, padding=\"same\")  \n",
    "        self.flatten = layers.Reshape((-1, embed_dim))  # Flatten to create Transformer tokens\n",
    "\n",
    "        # 2️. Transformer Encoder \n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads) for _ in range(depth)\n",
    "        ]\n",
    "\n",
    "        # 3️. Minibatch Discrimination (if enabled)\n",
    "        if self.use_minibatch:\n",
    "            self.minibatch_layer = MinibatchDiscrimination(num_kernels=100, dim_per_kernel=5)\n",
    "\n",
    "        # 4️. Normalization layer & MLP output head \n",
    "        self.norm = layers.LayerNormalization()   # Normalize features before final output\n",
    "        self.mlp_head = layers.Dense(1, activation=\"sigmoid\")  # Output layer for predicting real/fake\n",
    "\n",
    "    def call(self, inputs, training=False, return_features=False):\n",
    "        \"\"\"\n",
    "        Forward pass: if `return_features=True`, returns (output, features), otherwise just returns output.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(inputs)[0]  # Get the batch size\n",
    "\n",
    "        # Normalize image to the range [-1, 1]\n",
    "        inputs = (inputs - 0.5) * 2\n",
    "\n",
    "        # Patch Embedding\n",
    "        x = self.patch_embedding(inputs)  # (B, H, W, C) after convolution  \n",
    "        h, w, c = tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]  # Get height, width, and channels\n",
    "        x = tf.reshape(x, [batch_size, h * w, c])  # (B, num_patches, embed_dim) \n",
    "\n",
    "\n",
    "        # Transformer Encoder \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Average Pooling to get features \n",
    "        features = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "        # Minibatch Discrimination\n",
    "        if self.use_minibatch:\n",
    "            features = self.minibatch_layer(features)\n",
    "\n",
    "        # MLP Classification head  \n",
    "        x = self.norm(features)\n",
    "        output = self.mlp_head(x)\n",
    "\n",
    "        if return_features:\n",
    "            return output, features  # Return both classification result and features\n",
    "        else:\n",
    "            return output  # Only return classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Matching\n",
    "class FeatureMatching:\n",
    "    def __call__(self, real_features, fake_features):\n",
    "        real_mean = tf.reduce_mean(real_features, axis=0)\n",
    "        fake_mean = tf.reduce_mean(fake_features, axis=0)\n",
    "        return tf.reduce_mean(tf.square(real_mean - fake_mean))\n",
    "\n",
    "# Historical Averaging\n",
    "class HistoricalAveraging:\n",
    "    def __init__(self, beta=0.99):\n",
    "        self.beta = beta\n",
    "        self.parameter_history = {}\n",
    "\n",
    "    def initialize_if_needed(self, model):\n",
    "        for weight in model.trainable_weights:\n",
    "            if weight.name not in self.parameter_history:\n",
    "                self.parameter_history[weight.name] = weight.numpy()\n",
    "\n",
    "    def __call__(self, model, weight=0.01):\n",
    "        if not self.parameter_history:\n",
    "            return tf.constant(0.0)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for curr_weight in model.trainable_weights:\n",
    "            name = curr_weight.name\n",
    "            if name not in self.parameter_history:\n",
    "                continue\n",
    "            hist_tensor = tf.convert_to_tensor(self.parameter_history[name], dtype=curr_weight.dtype)\n",
    "            diff = curr_weight - hist_tensor\n",
    "            loss = tf.reduce_sum(tf.square(diff))\n",
    "            total_loss += loss\n",
    "        return weight * total_loss\n",
    "\n",
    "    def update_history(self, model):\n",
    "        for weight in model.trainable_weights:\n",
    "            name = weight.name\n",
    "            if name in self.parameter_history:\n",
    "                self.parameter_history[name] = self.beta * self.parameter_history[name] + (1 - self.beta) * weight.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ImprovedTransGAN Model**\n",
    "\n",
    "The ImprovedTransGAN class implements a Transformer-based Generative Adversarial Network (GAN) with several enhancements for stability and performance. Unlike traditional convolutional GANs, this model leverages self-attention mechanisms to capture long-range dependencies in image generation. By incorporating these techniques, ImprovedTransGAN achieves more stable training, higher-quality image synthesis, and improved generalization compared to traditional GANs. \n",
    "\n",
    "TransGAN Architecture:\n",
    "\n",
    "1. GridAttention-Based Generator: Uses transformers(GridAttention Block layers) instead of convolutional layers for better spatial understanding.\n",
    "\n",
    "2. Transformer-Based Discriminator: Processes image patches as tokens for superior global feature extraction.\n",
    "\n",
    "\n",
    "This implementation includes three major improvements over the TransGAN:\n",
    "\n",
    "1. Feature Matching – Improves training stability by aligning real and generated feature distributions.\n",
    "\n",
    "2. Historical Averaging – Stabilizes parameter updates by maintaining a history of model weights.\n",
    "\n",
    "3. Virtual Batch Normalization (VBN) – Normalizes activations using a fixed reference batch for improved generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTransGAN:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.generator = Generator(\n",
    "            base_size=config.base_size,\n",
    "            n_colors=3,\n",
    "            embed_dim=1024,\n",
    "            depths=(5, 4, 2),\n",
    "            num_heads=4,\n",
    "            ff_ratio=4,\n",
    "            latent_dim=config.z_dim\n",
    "        )\n",
    "        self.discriminator = TransformerDiscriminator(config)\n",
    "\n",
    "        self.feature_matching = FeatureMatching()\n",
    "        self.historical_averaging = HistoricalAveraging(beta=0.99)\n",
    "\n",
    "        self.gen_optimizer = tf.keras.optimizers.Adam(config.learning_rate_g, beta_1=config.beta1, beta_2=config.beta2)\n",
    "        self.disc_optimizer = tf.keras.optimizers.Adam(config.learning_rate_d, beta_1=config.beta1, beta_2=config.beta2)\n",
    "\n",
    "        os.makedirs(config.sample_dir, exist_ok=True)\n",
    "        os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "        self.fixed_noise = tf.random.normal([16, config.z_dim])\n",
    "        print(\"TransGAN model initialized!!!!!\")\n",
    "\n",
    "        self.gen_losses = []\n",
    "        self.disc_losses = []\n",
    "        self.real_scores = []\n",
    "        self.fake_scores = []\n",
    "\n",
    "    def generator_loss(self, fake_output, real_features=None, fake_features=None):\n",
    "        target = tf.ones_like(fake_output) * self.config.generator_target_prob\n",
    "        gen_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(target, fake_output, from_logits=True))\n",
    "\n",
    "        if self.config.use_feature_matching:\n",
    "            fm_loss = self.feature_matching(real_features, fake_features)\n",
    "            gen_loss += self.config.feature_matching_weight * fm_loss\n",
    "\n",
    "        if self.config.use_historical_averaging:\n",
    "            ha_loss = self.historical_averaging(self.generator, self.config.historical_averaging_weight)\n",
    "            gen_loss += ha_loss\n",
    "\n",
    "        return gen_loss\n",
    "\n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_labels = tf.ones_like(real_output) * (1.0 - self.config.label_smoothing)\n",
    "        fake_labels = tf.zeros_like(fake_output)\n",
    "\n",
    "        real_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(real_labels, real_output, from_logits=True))\n",
    "        fake_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(fake_labels, fake_output, from_logits=True))\n",
    "        disc_loss = real_loss + fake_loss\n",
    "\n",
    "        if self.config.use_historical_averaging:\n",
    "            ha_loss = self.historical_averaging(self.discriminator, self.config.historical_averaging_weight)\n",
    "            disc_loss += ha_loss\n",
    "\n",
    "        return disc_loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        noise = tf.random.normal([batch_size, self.config.z_dim])\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            fake_images = self.generator(noise, training=True)\n",
    "            real_output, real_features = self.discriminator(real_images, training=True, return_features=True)\n",
    "            fake_output, fake_features = self.discriminator(fake_images, training=True, return_features=True)\n",
    "            disc_loss = self.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        disc_gradients = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))\n",
    "        self.historical_averaging.update_history(self.discriminator)\n",
    "\n",
    "        real_score = tf.reduce_mean(real_output)\n",
    "        fake_score = tf.reduce_mean(fake_output)\n",
    "\n",
    "        gen_loss = 0.0\n",
    "        for _ in range(self.config.generator_steps):\n",
    "            with tf.GradientTape() as gen_tape:\n",
    "                fake_images = self.generator(noise, training=True)\n",
    "                fake_output, fake_features = self.discriminator(fake_images, training=True, return_features=True)\n",
    "                curr_gen_loss = self.generator_loss(fake_output, real_features, fake_features)\n",
    "                gen_loss += curr_gen_loss\n",
    "\n",
    "            gen_gradients = gen_tape.gradient(curr_gen_loss, self.generator.trainable_variables)\n",
    "            self.gen_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
    "            self.historical_averaging.update_history(self.generator)\n",
    "\n",
    "        return {\n",
    "            \"gen_loss\": gen_loss / self.config.generator_steps,\n",
    "            \"disc_loss\": disc_loss,\n",
    "            \"real_score\": real_score,\n",
    "            \"fake_score\": fake_score\n",
    "        }\n",
    "\n",
    "    def train(self, dataset, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for batch in dataset:\n",
    "                metrics = self.train_step(batch)\n",
    "\n",
    "            self.gen_losses.append(metrics['gen_loss'].numpy())\n",
    "            self.disc_losses.append(metrics['disc_loss'].numpy())\n",
    "            self.real_scores.append(metrics['real_score'].numpy())\n",
    "            self.fake_scores.append(metrics['fake_score'].numpy())\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - G_Loss: {metrics['gen_loss']:.4f}, D_Loss: {metrics['disc_loss']:.4f}\")\n",
    "\n",
    "            if (epoch + 1) % self.config.sample_freq == 0:\n",
    "                self.generate_images(epoch + 1)\n",
    "\n",
    "            if (epoch + 1) % self.config.save_freq == 0:\n",
    "                self.save_checkpoint(epoch + 1)\n",
    "\n",
    "    def generate_images(self, epoch):\n",
    "        predictions = self.generator(self.fixed_noise, training=False)\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            ax.imshow((predictions[i].numpy() + 1) / 2.0)\n",
    "            ax.axis(\"off\")\n",
    "        plt.savefig(f\"{self.config.sample_dir}/epoch_{epoch}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        epochs = range(1, len(self.gen_losses) + 1)\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.gen_losses, label=\"Generator Loss\")\n",
    "        plt.plot(epochs, self.disc_losses, label=\"Discriminator Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss Over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.real_scores, label=\"D(x) - Real\")\n",
    "        plt.plot(epochs, self.fake_scores, label=\"D(G(z)) - Fake\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.title(\"Discriminator Scores\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.config.sample_dir}/training_history.png\")\n",
    "        plt.show()\n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        checkpoint_dir = os.path.join(self.config.checkpoint_dir, f\"checkpoint_epoch_{epoch}\")\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        self.generator.save_weights(os.path.join(checkpoint_dir, \"generator.weights.h5\"))\n",
    "        self.discriminator.save_weights(os.path.join(checkpoint_dir, \"discriminator.weights.h5\"))\n",
    "\n",
    "    def load_checkpoint(self, epoch):\n",
    "        checkpoint_dir = os.path.join(self.config.checkpoint_dir, f\"checkpoint_epoch_{epoch}\")\n",
    "        self.generator.load_weights(os.path.join(checkpoint_dir, \"generator.weights.h5\"))\n",
    "        self.discriminator.load_weights(os.path.join(checkpoint_dir, \"discriminator.weights.h5\"))\n",
    "        print(f\"Checkpoint loaded from epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ImprovedTransGAN Model Runs like these:\n",
    "\n",
    "🔹 Step 1: Model Initialization\n",
    "\n",
    "Sets up the generator, discriminator, optimizers, additional stabilization techniques. And loads advanced training features.\n",
    "\n",
    "1.6 Creates directories for model checkpoints and sample images\n",
    "\n",
    "1.7 Generates fixed noise samples for evaluation during training.\n",
    "\n",
    "🔹 Step 2: Training Loop Execution\n",
    "\n",
    "1. Processes real images in mini-batches.\n",
    "\n",
    "2. Performs a discriminator update: (1) Uses multi-head self-attention to analyze image patches. (2)Includes Minibatch Discrimination (optional) to prevent mode collapse.\n",
    "\n",
    "3. Performs multiple generator updates (generator_steps): (1)Uses self-attention instead of CNN layers. (2)Projects latent noise into a structured spatial feature map. (3)Incorporates Virtual Batch Normalization (VBN) for training stability.\n",
    "\n",
    "4. Configures advanced training techniques: (1)Feature Matching: Ensures feature distributions of real and fake images are similar. (2)Historical Averaging: Uses past model weights to stabilize learning.\n",
    "\n",
    "5. Sets up optimizers (Adam) for both networks: (1)Separate learning rates for generator and discriminator. (2)Uses momentum parameters (beta1, beta2) for stable updates.\n",
    "\n",
    "6. Logs training performance. \n",
    "\n",
    "7. Generates sample images every sample_freq epochs.\n",
    "\n",
    "8. Saves checkpoints every save_freq epochs.\n",
    "\n",
    "🔹 Step 3: Evaluating Model Performance\n",
    "\n",
    "1. Plots training loss curves and discriminator confidence scores.\n",
    "\n",
    "2. Generates synthetic images to monitor improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIDEvaluator**\n",
    "\n",
    "The FIDEvaluator class is designed to evaluate the performance of a Generative Adversarial Network (GAN) by calculating the Fréchet Inception Distance (FID). The FID score is a widely used metric to measure the similarity between real and generated images, helping assess the quality of synthetic data. By leveraging InceptionV3 to extract meaningful image features, it provides a robust and reliable measure of image realism. This metric is fundamental in improving and benchmarking GAN-based image generation models.\n",
    "\n",
    "The key idea behind FID is to compare the distributions of real and generated images in a high-level feature space extracted using a pre-trained InceptionV3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIDEvaluator:\n",
    "    \"\"\"Evaluates GAN performance using Fréchet Inception Distance (FID).\n",
    "    FID measures the similarity between generated and real image distributions\n",
    "    using features extracted from a pre-trained InceptionV3 model.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.batch_size = config.batch_size\n",
    "\n",
    "        self.inception_model = tf.keras.applications.InceptionV3(\n",
    "            include_top=False,\n",
    "            pooling='avg',\n",
    "            weights='imagenet',\n",
    "            input_shape=(299, 299, 3)\n",
    "        )\n",
    "\n",
    "        self.real_features = None\n",
    "        self.real_mean = None\n",
    "        self.real_cov = None\n",
    "\n",
    "        print(\"FID Evaluator initialized\")\n",
    "\n",
    "    def preprocess_images(self, images):\n",
    "        images = (images + 1) / 2.0\n",
    "        images = tf.image.resize(images, (299, 299))\n",
    "        images = tf.keras.applications.inception_v3.preprocess_input(images * 255.0)\n",
    "        return images\n",
    "\n",
    "    def extract_features(self, images):\n",
    "        features = self.inception_model.predict(images, batch_size=self.batch_size)\n",
    "        return features\n",
    "\n",
    "    def compute_real_statistics(self, dataset, num_samples=10000):\n",
    "        print(f\"Computing real data statistics using {num_samples} samples...\")\n",
    "\n",
    "        real_images = []\n",
    "        for batch in dataset:\n",
    "            for img in batch:\n",
    "                real_images.append(img.numpy())\n",
    "            if len(real_images) >= num_samples:\n",
    "                break\n",
    "\n",
    "        real_images = np.array(real_images[:num_samples])\n",
    "        processed_images = self.preprocess_images(real_images)\n",
    "\n",
    "        features = []\n",
    "        batch_size = self.batch_size\n",
    "        for i in range(0, len(processed_images), batch_size):\n",
    "            batch = processed_images[i:i+batch_size]\n",
    "            batch_features = self.extract_features(batch)\n",
    "            features.append(batch_features)\n",
    "\n",
    "        self.real_features = np.concatenate(features, axis=0)\n",
    "\n",
    "        self.real_mean = np.mean(self.real_features, axis=0)\n",
    "        self.real_cov = np.cov(self.real_features, rowvar=False)\n",
    "\n",
    "        print(f\"Real data statistics calculated from {len(self.real_features)} images\")\n",
    "\n",
    "        return self.real_mean, self.real_cov\n",
    "\n",
    "    def calculate_fid(self, generator, num_samples=10000):\n",
    "        if self.real_mean is None or self.real_cov is None:\n",
    "            raise ValueError(\"Real statistics not computed. Run compute_real_statistics first.\")\n",
    "\n",
    "        print(f\"Calculating FID score using {num_samples} generated samples...\")\n",
    "\n",
    "        z_dim = self.config.z_dim\n",
    "        batch_size = self.batch_size\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "        fake_images = []\n",
    "        for i in range(num_batches):\n",
    "            current_batch_size = min(batch_size, num_samples - i * batch_size)\n",
    "            z = tf.random.normal([current_batch_size, z_dim])\n",
    "            generated_batch = generator(z, training=False)\n",
    "            fake_images.append(generated_batch)\n",
    "\n",
    "        fake_images = np.concatenate(fake_images, axis=0)[:num_samples]\n",
    "        processed_images = self.preprocess_images(fake_images)\n",
    "\n",
    "        fake_features = []\n",
    "        for i in range(0, len(processed_images), batch_size):\n",
    "            batch = processed_images[i:i+batch_size]\n",
    "            batch_features = self.extract_features(batch)\n",
    "            fake_features.append(batch_features)\n",
    "\n",
    "        fake_features = np.concatenate(fake_features, axis=0)\n",
    "\n",
    "        fake_mean = np.mean(fake_features, axis=0)\n",
    "        fake_cov = np.cov(fake_features, rowvar=False)\n",
    "\n",
    "        from scipy import linalg\n",
    "\n",
    "        mean_diff_squared = np.sum((self.real_mean - fake_mean) ** 2)\n",
    "        covmean = linalg.sqrtm(self.real_cov.dot(fake_cov))\n",
    "\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "\n",
    "        fid = mean_diff_squared + np.trace(self.real_cov + fake_cov - 2 * covmean)\n",
    "\n",
    "        print(f\"FID Score: {fid:.4f} (lower is better)\")\n",
    "\n",
    "        return fid\n",
    "\n",
    "    def evaluate_with_samples(self, real_images, fake_images, num_samples=10000):\n",
    "        real_samples = real_images[:num_samples]\n",
    "        fake_samples = fake_images[:num_samples]\n",
    "\n",
    "        processed_real = self.preprocess_images(real_samples)\n",
    "        processed_fake = self.preprocess_images(fake_samples)\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        real_features = []\n",
    "        for i in range(0, len(processed_real), batch_size):\n",
    "            batch = processed_real[i:i+batch_size]\n",
    "            batch_features = self.extract_features(batch)\n",
    "            real_features.append(batch_features)\n",
    "        real_features = np.concatenate(real_features, axis=0)\n",
    "\n",
    "        fake_features = []\n",
    "        for i in range(0, len(processed_fake), batch_size):\n",
    "            batch = processed_fake[i:i+batch_size]\n",
    "            batch_features = self.extract_features(batch)\n",
    "            fake_features.append(batch_features)\n",
    "        fake_features = np.concatenate(fake_features, axis=0)\n",
    "\n",
    "        real_mean = np.mean(real_features, axis=0)\n",
    "        real_cov = np.cov(real_features, rowvar=False)\n",
    "\n",
    "        fake_mean = np.mean(fake_features, axis=0)\n",
    "        fake_cov = np.cov(fake_features, rowvar=False)\n",
    "\n",
    "        from scipy import linalg\n",
    "\n",
    "        mean_diff_squared = np.sum((real_mean - fake_mean) ** 2)\n",
    "        covmean = linalg.sqrtm(real_cov.dot(fake_cov))\n",
    "\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "\n",
    "        fid = mean_diff_squared + np.trace(real_cov + fake_cov - 2 * covmean)\n",
    "\n",
    "        print(f\"FID Score: {fid:.4f} (lower is better)\")\n",
    "\n",
    "        return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments for ImprovedTransGAN\n",
    "def run_transgan_experiment(config, dataset, epochs=10, evaluate_every=5):\n",
    "    print(\"=== Starting TransGAN Experiment ===\")\n",
    "    print(f\"Configuration: {config.__dict__}\")\n",
    "    print(f\"Training for {epochs} epochs, evaluating every {evaluate_every} epochs\")\n",
    "\n",
    "    # Initialize the FID evaluator and compute statistics for real images\n",
    "    fid_evaluator = FIDEvaluator(config)\n",
    "    fid_evaluator.compute_real_statistics(dataset, num_samples=1000)\n",
    "\n",
    "    # Initialize the ImprovedTransGAN model\n",
    "    gan = ImprovedTransGAN(config)\n",
    "    fid_scores = []   # List to store FID scores for evaluation\n",
    "\n",
    "    # Create log directory for storing results\n",
    "    os.makedirs(config.log_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{epochs} ===\")\n",
    "        gan.train(dataset, 1)   # Train the GAN for one epoch \n",
    "\n",
    "        # Periodically evaluate FID  \n",
    "        if (epoch + 1) % evaluate_every == 0 or epoch == epochs - 1:\n",
    "            print(f\"Evaluating FID after epoch {epoch+1}\")\n",
    "            fid = fid_evaluator.calculate_fid(gan.generator, num_samples=1000)\n",
    "            fid_scores.append((epoch + 1, fid))\n",
    "\n",
    "    # Final FID evaluation \n",
    "    print(\"\\n=== Final Evaluation ===\")\n",
    "    final_fid = fid_evaluator.calculate_fid(gan.generator, num_samples=2000)\n",
    "\n",
    "    # Plot FID scores over training epochs\n",
    "    if len(fid_scores) > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs_, scores = zip(*fid_scores)  # Unzip epoch and score tuples \n",
    "        plt.plot(epochs_, scores, 'o-', linewidth=2)  # Plot the FID scores\n",
    "        plt.title('FID Score Over Training (Lower is Better)')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('FID Score')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f\"{config.log_dir}/fid_history.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Display final generated samples\n",
    "    final_samples = 36  # Number of samples to generate\n",
    "    noise = tf.random.normal([final_samples, config.z_dim])  # Generate random noise vectors\n",
    "    generated_images = gan.generator(noise, training=False)  # Generate images from noise \n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(final_samples):\n",
    "        plt.subplot(6, 6, i+1)  # Create a grid for displaying generated images\n",
    "        img = (generated_images[i].numpy() + 1) / 2.0\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f'Final Generated Samples (FID: {final_fid:.4f})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.log_dir}/final_samples.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return gan, fid_scores, final_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransGANConfig:\n",
    "    def __init__(self):\n",
    "        self.z_dim = 128  # Dimension of the latent space (noise vector)\n",
    "        self.base_size = 4  # Base size for generator and discriminator (must be set) \n",
    "        self.learning_rate_g = 1e-4  # Learning rate for the generator\n",
    "        self.learning_rate_d = 1e-4  # Learning rate for the discriminator\n",
    "        self.beta1 = 0.5  # Adam optimizer beta1 parameter\n",
    "        self.beta2 = 0.999  # Adam optimizer beta2 parameter \n",
    "        self.generator_steps = 1  # Number of generator updates per discriminator update \n",
    "\n",
    "        self.use_feature_matching = False  # Flag for enabling feature matching\n",
    "        self.use_historical_averaging = False  # Flag for enabling historical averaging\n",
    "        self.feature_matching_weight = 1.0  # Weight for the feature matching loss\n",
    "        self.historical_averaging_weight = 0.1  # Weight for the historical averaging loss\n",
    "\n",
    "        self.label_smoothing = 0.0  # Smoothing factor for labels to help with training \n",
    "        self.generator_target_prob = 0.9 # Target probability for generator outputs  \n",
    "\n",
    "        self.sample_freq = 1 # Frequency of generating samples  \n",
    "        self.save_freq = 5  # Frequency of saving checkpoints \n",
    "\n",
    "        self.sample_dir = \"./samples\" # Directory for saving generated samples \n",
    "        self.checkpoint_dir = \"./checkpoints\" # Directory for saving model checkpoints  \n",
    "        self.log_dir = \"./logs\" # Directory for saving training logs \n",
    "\n",
    "        self.batch_size = 64  # ⚠️ Used for FIDEvaluator  \n",
    "\n",
    "\n",
    "def compare_transgan_improvements(dataset, base_epochs=5):\n",
    "    print(\"=== TransGAN Improvement Comparison Study ===\")\n",
    "\n",
    "    # List of different configurations for comparison\n",
    "    configs = [\n",
    "        {\n",
    "            \"name\": \"baseline\",\n",
    "            \"updates\": {\n",
    "                \"use_feature_matching\": False,\n",
    "                \"use_historical_averaging\": False,\n",
    "                \"label_smoothing\": 0.0\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"feature_matching\",\n",
    "            \"updates\": {\n",
    "                \"use_feature_matching\": True,\n",
    "                \"use_historical_averaging\": False,\n",
    "                \"label_smoothing\": 0.0\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"historical_avg\",\n",
    "            \"updates\": {\n",
    "                \"use_feature_matching\": False,\n",
    "                \"use_historical_averaging\": True,\n",
    "                \"label_smoothing\": 0.0\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"label_smoothing\",\n",
    "            \"updates\": {\n",
    "                \"use_feature_matching\": False,\n",
    "                \"use_historical_averaging\": False,\n",
    "                \"label_smoothing\": 0.1\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"all_improvements\",\n",
    "            \"updates\": {\n",
    "                \"use_feature_matching\": True,\n",
    "                \"use_historical_averaging\": True,\n",
    "                \"label_smoothing\": 0.1\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    base_config = TransGANConfig() # Initialize the base configuration \n",
    "    fid_evaluator = FIDEvaluator(base_config) # Initialize the FID evaluator\n",
    "    fid_evaluator.compute_real_statistics(dataset, num_samples=1000) # Compute statistics for real images  \n",
    "\n",
    "    results = {}  # Dictionary to store results for each configuration\n",
    "\n",
    "    for config_info in configs:\n",
    "        name = config_info[\"name\"]  # Get the configuration name\n",
    "        updates = config_info[\"updates\"]  # Get the configuration updates\n",
    "\n",
    "        print(f\"\\n\\n=== Training Configuration: {name} ===\")\n",
    "        for k, v in updates.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "        config = TransGANConfig()\n",
    "        for k, v in updates.items():\n",
    "            setattr(config, k, v)\n",
    "\n",
    "        config.sample_dir = f\"samples/{name}\"\n",
    "        config.checkpoint_dir = f\"checkpoints/{name}\"\n",
    "        config.log_dir = f\"logs/{name}\"\n",
    "\n",
    "        os.makedirs(config.sample_dir, exist_ok=True)\n",
    "        os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(config.log_dir, exist_ok=True)\n",
    "\n",
    "        gan = ImprovedTransGAN(config)\n",
    "        gan.train(dataset, base_epochs)\n",
    "\n",
    "        fid_score = fid_evaluator.calculate_fid(gan.generator, num_samples=1000)\n",
    "\n",
    "        results[name] = {\n",
    "            \"fid\": fid_score,\n",
    "            \"gan\": gan,\n",
    "            \"config\": config,\n",
    "            \"gen_loss\": gan.gen_losses[-1] if gan.gen_losses else None,\n",
    "            \"disc_loss\": gan.disc_losses[-1] if gan.disc_losses else None,\n",
    "            \"real_score\": gan.real_scores[-1] if gan.real_scores else None,\n",
    "            \"fake_score\": gan.fake_scores[-1] if gan.fake_scores else None\n",
    "        }\n",
    "\n",
    "        np.savez(\n",
    "            f\"{config.log_dir}/metrics.npz\",\n",
    "            gen_losses=np.array(gan.gen_losses),\n",
    "            disc_losses=np.array(gan.disc_losses),\n",
    "            real_scores=np.array(gan.real_scores),\n",
    "            fake_scores=np.array(gan.fake_scores),\n",
    "            fid=np.array([fid_score])\n",
    "        )\n",
    "\n",
    " \n",
    "    names = list(results.keys())\n",
    "    fids = [results[n][\"fid\"] for n in names]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(names, fids, color='skyblue')\n",
    "    plt.title(\"FID Scores for Different TransGAN Improvements\")\n",
    "    plt.ylabel(\"FID (Lower is Better)\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"transgan_improvement_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def visualize_comparison_results(results):\n",
    "    print(\"Creating comparative visualizations...\")\n",
    "\n",
    "    # Automatically assign colors for different configurations  \n",
    "    default_colors = itertools.cycle(['gray', 'blue', 'green', 'orange', 'purple', 'red', 'cyan', 'magenta'])\n",
    "    colors = {name: next(default_colors) for name in results}\n",
    "\n",
    "    # --- FID Bar Chart ---  \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    names = list(results.keys())\n",
    "    fid_scores = [results[name][\"fid\"] for name in names]\n",
    "\n",
    "    sorted_indices = np.argsort(fid_scores)\n",
    "    sorted_names = [names[i] for i in sorted_indices]\n",
    "    sorted_fids = [fid_scores[i] for i in sorted_indices]\n",
    "    sorted_colors = [colors[name] for name in sorted_names]\n",
    "\n",
    "    plt.barh(sorted_names, sorted_fids, color=sorted_colors)\n",
    "    plt.title('FID Score Comparison (Lower is Better)')\n",
    "    plt.xlabel('FID Score')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    for i, v in enumerate(sorted_fids):\n",
    "        plt.text(v + 0.5, i, f\"{v:.2f}\", va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"fid_comparison.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Loss Curve Comparison ---  \n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name in results:\n",
    "        if hasattr(results[name][\"gan\"], \"gen_losses\"):\n",
    "            plt.plot(results[name][\"gan\"].gen_losses, label=name, color=colors[name], linewidth=2)\n",
    "    plt.title('Generator Loss Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name in results:\n",
    "        if hasattr(results[name][\"gan\"], \"disc_losses\"):\n",
    "            plt.plot(results[name][\"gan\"].disc_losses, label=name, color=colors[name], linewidth=2)\n",
    "    plt.title('Discriminator Loss Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"loss_comparison.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Discriminator Score Comparison --- \n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name in results:\n",
    "        if hasattr(results[name][\"gan\"], \"real_scores\"):\n",
    "            plt.plot(results[name][\"gan\"].real_scores, label=name, color=colors[name], linewidth=2)\n",
    "    plt.title('D(x) - Real Score Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name in results:\n",
    "        if hasattr(results[name][\"gan\"], \"fake_scores\"):\n",
    "            plt.plot(results[name][\"gan\"].fake_scores, label=name, color=colors[name], linewidth=2)\n",
    "    plt.title('D(G(z)) - Fake Score Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"score_comparison.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Output generated images for each model ---  \n",
    "    for name, result in results.items():\n",
    "        config = result[\"config\"]\n",
    "        z_dim = config.z_dim\n",
    "        fixed_noise = tf.random.normal([16, z_dim])\n",
    "\n",
    "        generated_images = result[\"gan\"].generator(fixed_noise, training=False)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.suptitle(f\"{name.replace('_', ' ').title()} (FID: {result['fid']:.2f})\", fontsize=14)\n",
    "\n",
    "        for j in range(16):\n",
    "            plt.subplot(4, 4, j+1)\n",
    "            img = (generated_images[j].numpy() + 1) / 2.0\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "\n",
    "        save_path = os.path.join(config.log_dir, f\"samples_{name}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "    # --- Output summary table of results --- \n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Configuration':<20} {'FID Score':<12} {'Gen Loss':<12} {'Disc Loss':<12} {'D(x)':<12} {'D(G(z))':<12}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1][\"fid\"])\n",
    "\n",
    "    for name, result in sorted_results:\n",
    "        print(f\"{name:<20} {result['fid']:<12.4f} \", end=\"\")\n",
    "        print(f\"{result.get('gen_loss', 'N/A'):<12.4f} \" if result.get('gen_loss') is not None else f\"{'N/A':<12} \", end=\"\")\n",
    "        print(f\"{result.get('disc_loss', 'N/A'):<12.4f} \" if result.get('disc_loss') is not None else f\"{'N/A':<12} \", end=\"\")\n",
    "        print(f\"{result.get('real_score', 'N/A'):<12.4f} \" if result.get('real_score') is not None else f\"{'N/A':<12} \", end=\"\")\n",
    "        print(f\"{result.get('fake_score', 'N/A'):<12.4f}\" if result.get('fake_score') is not None else f\"{'N/A':<12}\")\n",
    "\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy config class that includes a z_dim attribute for testing purposes  \n",
    "class DummyConfig:\n",
    "    def __init__(self, z_dim=100):\n",
    "        self.z_dim = z_dim\n",
    "        self.log_dir = \"./logs\"\n",
    "\n",
    "# Dummy GAN class to simulate the structure of a real GAN for testing  \n",
    "class DummyGAN:\n",
    "    def __init__(self):\n",
    "        self.gen_losses = [np.random.uniform(2.0, 3.0) for _ in range(3)]\n",
    "        self.disc_losses = [np.random.uniform(1.0, 2.0) for _ in range(3)]\n",
    "        self.real_scores = [np.random.uniform(0.6, 0.8) for _ in range(3)]\n",
    "        self.fake_scores = [np.random.uniform(0.2, 0.4) for _ in range(3)]\n",
    "        self.generator = lambda z, training=False: tf.random.uniform([z.shape[0], 16, 16, 3], -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mode_collapse(results, dataset, num_samples=1000):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "\n",
    "    print(\"Analyzing mode collapse across configurations...\")\n",
    "\n",
    "    # Collect real images from the dataset \n",
    "    real_images = []\n",
    "    for batch in dataset:\n",
    "        for img in batch:\n",
    "            real_images.append(img.numpy())\n",
    "        if len(real_images) >= num_samples:\n",
    "            break\n",
    "\n",
    "    real_images = np.array(real_images[:num_samples])\n",
    "\n",
    "    # Initialize InceptionV3 model for feature extraction  \n",
    "    inception_model = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        pooling='avg',\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Function to preprocess images for the Inception model  \n",
    "    def preprocess_images(images):\n",
    "        images = (images + 1) / 2.0\n",
    "        images = tf.image.resize(images, (299, 299))\n",
    "        images = tf.keras.applications.inception_v3.preprocess_input(images * 255.0)\n",
    "        return images\n",
    "    \n",
    "    # Preprocess real images and extract features using Inception model \n",
    "    processed_real = preprocess_images(real_images)\n",
    "    real_features = inception_model.predict(processed_real, batch_size=32, verbose=0)\n",
    "\n",
    "    feature_stats = {}\n",
    "\n",
    "    # Loop through results to analyze each configuration  \n",
    "    for name, result in results.items():\n",
    "        config = result[\"config\"]\n",
    "        z = tf.random.normal([num_samples, config.z_dim])\n",
    "        fake_images = result[\"gan\"].generator(z, training=False).numpy()\n",
    "\n",
    "        # Preprocess generated images and extract features \n",
    "        processed_fake = preprocess_images(fake_images)\n",
    "        fake_features = inception_model.predict(processed_fake, batch_size=32, verbose=0)\n",
    "\n",
    "        # Store feature statistics for generated images \n",
    "        feature_stats[name] = {\n",
    "            \"features\": fake_features,\n",
    "            \"mean\": np.mean(fake_features, axis=0),\n",
    "            \"std\": np.std(fake_features, axis=0),\n",
    "            \"min\": np.min(fake_features, axis=0),\n",
    "            \"max\": np.max(fake_features, axis=0)\n",
    "        }\n",
    "\n",
    "    \n",
    "    feature_diversity = {}\n",
    "    for name, stats in feature_stats.items():\n",
    "        avg_std = np.mean(stats[\"std\"])\n",
    "        avg_range = np.mean(stats[\"max\"] - stats[\"min\"])\n",
    "\n",
    "        feature_diversity[name] = {\n",
    "            \"avg_std\": avg_std,\n",
    "            \"avg_range\": avg_range\n",
    "        }\n",
    "\n",
    "    # Calculate statistics for real images  \n",
    "    real_std = np.mean(np.std(real_features, axis=0))\n",
    "    real_range = np.mean(np.max(real_features, axis=0) - np.min(real_features, axis=0))\n",
    "\n",
    "    # Plot feature diversity \n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    names = list(feature_diversity.keys())\n",
    "    avg_stds = [feature_diversity[name][\"avg_std\"] for name in names]\n",
    "\n",
    "    # Sort feature diversities in descending order  \n",
    "    sorted_indices = np.argsort(avg_stds)[::-1]\n",
    "    sorted_names = [names[i] for i in sorted_indices]\n",
    "    sorted_stds = [avg_stds[i] for i in sorted_indices]\n",
    "\n",
    "    # Assign colors to configurations for plotting\n",
    "    colors = {\n",
    "        'baseline': 'gray',\n",
    "        'feature_matching': 'blue',\n",
    "        'minibatch_disc': 'green',\n",
    "        'historical_avg': 'purple',\n",
    "        'label_smoothing': 'orange',\n",
    "        'all_improvements': 'red'\n",
    "    }\n",
    "    sorted_colors = [colors.get(name, 'skyblue') for name in sorted_names]\n",
    "\n",
    "    plt.bar(sorted_names, sorted_stds, color=sorted_colors)\n",
    "    plt.axhline(y=real_std, color='black', linestyle='--', label=f'Real Data ({real_std:.4f})')\n",
    "\n",
    "    plt.title('Feature Diversity Comparison (Higher is Better)')\n",
    "    plt.ylabel('Average Feature Standard Deviation')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    for i, v in enumerate(sorted_stds):\n",
    "        plt.text(i, v + 0.001, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_diversity.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nMode Collapse Analysis:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Configuration':<20} {'Feature Std':<15} {'% of Real':<15} {'Mode Collapse':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    sorted_results = sorted(feature_diversity.items(), key=lambda x: x[1][\"avg_std\"], reverse=True)\n",
    "\n",
    "    for name, stats in sorted_results:\n",
    "        avg_std = stats[\"avg_std\"]\n",
    "        pct_of_real = (avg_std / real_std) * 100\n",
    "\n",
    "        if pct_of_real >= 90:\n",
    "            collapse_status = \"Minimal\"\n",
    "        elif pct_of_real >= 70:\n",
    "            collapse_status = \"Minor\"\n",
    "        elif pct_of_real >= 50:\n",
    "            collapse_status = \"Moderate\"\n",
    "        elif pct_of_real >= 30:\n",
    "            collapse_status = \"Significant\"\n",
    "        else:\n",
    "            collapse_status = \"Severe\"\n",
    "\n",
    "        print(f\"{name:<20} {avg_std:<15.4f} {pct_of_real:<15.2f}% {collapse_status:<15}\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Real Data Reference: {real_std:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(results, dataset_name, epochs):\n",
    "    print(\"\\n=== Comprehensive Experiment Summary ===\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Training epochs per configuration: {epochs}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    best_config = min(results.items(), key=lambda x: x[1][\"fid\"])\n",
    "    worst_config = max(results.items(), key=lambda x: x[1][\"fid\"])\n",
    "\n",
    "    print(f\"Best configuration: {best_config[0]} (FID: {best_config[1]['fid']:.4f})\")\n",
    "    print(f\"Worst configuration: {worst_config[0]} (FID: {worst_config[1]['fid']:.4f})\")\n",
    "    print(f\"Improvement: {(worst_config[1]['fid'] - best_config[1]['fid']) / worst_config[1]['fid'] * 100:.2f}%\")\n",
    "\n",
    "    print(\"\\n=== Impact of Individual Techniques ===\")\n",
    "\n",
    "    baseline_fid = results[\"baseline\"][\"fid\"]\n",
    "\n",
    "    techniques = [\"feature_matching\", \"minibatch_disc\", \"historical_avg\", \"label_smoothing\"]\n",
    "\n",
    "    for technique in techniques:\n",
    "        if technique in results:\n",
    "            technique_fid = results[technique][\"fid\"]\n",
    "            improvement = (baseline_fid - technique_fid) / baseline_fid * 100\n",
    "\n",
    "            print(f\"{technique.replace('_', ' ').title()}:\")\n",
    "            print(f\"  FID: {technique_fid:.4f}\")\n",
    "            print(f\"  Improvement over baseline: {improvement:.2f}%\")\n",
    "\n",
    "    if \"all_improvements\" in results:\n",
    "        all_fid = results[\"all_improvements\"][\"fid\"]\n",
    "        all_improvement = (baseline_fid - all_fid) / baseline_fid * 100\n",
    "\n",
    "        print(f\"\\nAll Improvements Combined:\")\n",
    "        print(f\"  FID: {all_fid:.4f}\")\n",
    "        print(f\"  Improvement over baseline: {all_improvement:.2f}%\")\n",
    "\n",
    "    print(\"\\n=== Conclusions ===\")\n",
    "\n",
    "    technique_improvements = []\n",
    "\n",
    "    for technique in techniques:\n",
    "        if technique in results:\n",
    "            technique_fid = results[technique][\"fid\"]\n",
    "            improvement = (baseline_fid - technique_fid) / baseline_fid * 100\n",
    "            technique_improvements.append((technique, improvement))\n",
    "\n",
    "    technique_improvements.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"Technique effectiveness ranking:\")\n",
    "    for i, (technique, improvement) in enumerate(technique_improvements):\n",
    "        print(f\"  {i+1}. {technique.replace('_', ' ').title()}: {improvement:.2f}% improvement\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_experiment(epochs=10, dataset_name='cifar10'):\n",
    "    print(f\"=== Starting Complete TransGAN Experiment ===\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Training epochs: {epochs}\")\n",
    "\n",
    "    class Config:\n",
    "        z_dim = 128\n",
    "        base_size = 4\n",
    "        learning_rate_g = 1e-4\n",
    "        learning_rate_d = 1e-4\n",
    "        beta1 = 0.5\n",
    "        beta2 = 0.999\n",
    "        generator_steps = 1\n",
    "        use_feature_matching = False\n",
    "        use_historical_averaging = False\n",
    "        use_minibatch_discrimination = False  # optional\n",
    "        feature_matching_weight = 1.0\n",
    "        historical_averaging_weight = 0.1\n",
    "        label_smoothing = 0.0\n",
    "        generator_target_prob = 0.9\n",
    "        sample_freq = 1\n",
    "        save_freq = 10\n",
    "        checkpoint_dir = \"./checkpoints\"\n",
    "        sample_dir = \"./samples\"\n",
    "        log_dir = \"./logs\"\n",
    "        batch_size = 64\n",
    "\n",
    "    config = Config()\n",
    "\n",
    "    # Load CIFAR10\n",
    "    (x_train, y_train), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "\n",
    "    def preprocess_image(img):\n",
    "      img = tf.image.resize(img, [32, 32])\n",
    "      img = tf.cast(img, tf.float32)\n",
    "      img = (img - 127.5) / 127.5\n",
    "      return img\n",
    "    \n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.batch(config.batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = compare_transgan_improvements(dataset, base_epochs=epochs)\n",
    "\n",
    "    analyze_mode_collapse(results, dataset)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal experiment time: {total_time/60:.2f} minutes\")\n",
    "\n",
    "    generate_summary_report(results, dataset_name, epochs)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_complete_experiment(epochs=3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
